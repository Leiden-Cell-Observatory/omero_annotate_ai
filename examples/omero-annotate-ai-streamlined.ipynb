{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OMERO Micro-SAM Streamlined Workflow\n",
    "\n",
    "This notebook demonstrates the streamlined 2-widget workflow using the `omero-annotate-ai` package.\n",
    "\n",
    "## Key Features:\n",
    "- **2 Widget Workflow**: Only connection and workflow widgets needed\n",
    "- **Sequential Process**: Step-by-step guided workflow\n",
    "- **Complete Integration**: Direct pipeline execution with `run_full_workflow()`\n",
    "- **Secure Connection**: Built-in keychain support and connection management\n",
    "\n",
    "## Process Overview:\n",
    "1. **OMERO Connection** - Secure connection with password management\n",
    "2. **Workflow Setup** - Sequential configuration process\n",
    "3. **Pipeline Execution** - Run the complete annotation workflow\n",
    "4. **Results** - View results and export configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "**Prerequisites:**\n",
    "```bash\n",
    "# Activate micro-sam environment\n",
    "conda activate micro-sam\n",
    "\n",
    "# Install the package\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "**Note**: The micro-sam dependency must be installed separately:\n",
    "```bash\n",
    "conda install -c conda-forge micro-sam\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ omero-annotate-ai version: 0.1.0\n",
      "ğŸ”— Available widgets: Connection, Workflow\n",
      "âœ¨ Streamlined 2-widget workflow ready!\n",
      "ğŸ”— OMERO functionality: âœ… Available\n",
      "ğŸ”‘ Keyring support: âœ… Available\n"
     ]
    }
   ],
   "source": [
    "# Import the streamlined package\n",
    "import omero_annotate_ai\n",
    "from omero_annotate_ai import (\n",
    "    create_omero_connection_widget,\n",
    "    create_workflow_widget,\n",
    "    create_pipeline\n",
    ")\n",
    "\n",
    "# System imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"ğŸ“¦ omero-annotate-ai version: {omero_annotate_ai.__version__}\")\n",
    "print(f\"ğŸ”— Available widgets: Connection, Workflow\")\n",
    "print(f\"âœ¨ Streamlined 2-widget workflow ready!\")\n",
    "\n",
    "# Check dependencies\n",
    "try:\n",
    "    import ezomero\n",
    "    print(f\"ğŸ”— OMERO functionality: âœ… Available\")\n",
    "except ImportError:\n",
    "    print(f\"ğŸ”— OMERO functionality: âŒ Install with: pip install -e .[omero]\")\n",
    "\n",
    "try:\n",
    "    import keyring\n",
    "    print(f\"ğŸ”‘ Keyring support: âœ… Available\")\n",
    "except ImportError:\n",
    "    print(f\"ğŸ”‘ Keyring support: âš ï¸ Not available (manual password entry only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. OMERO Connection\n",
    "\n",
    "Use the secure connection widget to connect to your OMERO server. This widget provides:\n",
    "- Auto-loading from `.env` and `.ezomero` files\n",
    "- Secure password storage in OS keychain\n",
    "- Connection testing and validation\n",
    "- Password expiration options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”Œ OMERO Connection Setup\n",
      "Use the widget below to connect to your OMERO server:\n",
      "  â€¢ Fill in server details (host, username, password)\n",
      "  â€¢ Test connection before proceeding\n",
      "  â€¢ Choose password storage duration if desired\n",
      "  â€¢ Click 'Save & Connect' to establish connection\n",
      "\n",
      "ğŸ“„ Loaded configuration from connection history: root@localhost\n",
      "ğŸ” Password loaded from keychain (no expiration)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e17d1434b57489e828b53d1d3dfa0a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>ğŸ”Œ OMERO Server Connection</h3>', layout=Layout(margin='0 0 20px 0')), HTML(valuâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Next Step: Run the cell below after connecting to OMERO\n"
     ]
    }
   ],
   "source": [
    "# Create and display the OMERO connection widget\n",
    "print(\"ğŸ”Œ OMERO Connection Setup\")\n",
    "print(\"Use the widget below to connect to your OMERO server:\")\n",
    "print(\"  â€¢ Fill in server details (host, username, password)\")\n",
    "print(\"  â€¢ Test connection before proceeding\")\n",
    "print(\"  â€¢ Choose password storage duration if desired\")\n",
    "print(\"  â€¢ Click 'Save & Connect' to establish connection\")\n",
    "print()\n",
    "\n",
    "conn_widget = create_omero_connection_widget()\n",
    "conn_widget.display()\n",
    "\n",
    "print(\"\\nğŸ“ Next Step: Run the cell below after connecting to OMERO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OMERO connection established!\n",
      "ğŸ‘¤ User: root\n",
      "ğŸ¢ Group: system\n",
      "ğŸ” Secure: True\n",
      "ğŸ”— Connection ready for workflow setup\n"
     ]
    }
   ],
   "source": [
    "# Get the OMERO connection\n",
    "conn = conn_widget.get_connection()\n",
    "\n",
    "if conn is None:\n",
    "    raise ConnectionError(\"âŒ No OMERO connection established. Please use the widget above to connect.\")\n",
    "\n",
    "print(\"âœ… OMERO connection established!\")\n",
    "print(f\"ğŸ‘¤ User: {conn.getUser().getName()}\")\n",
    "print(f\"ğŸ¢ Group: {conn.getGroupFromContext().getName()}\")\n",
    "print(f\"ğŸ” Secure: {conn.isSecure()}\")\n",
    "print(f\"ğŸ”— Connection ready for workflow setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Workflow Configuration\n",
    "\n",
    "The workflow widget provides a sequential, step-by-step process for configuring your annotation workflow:\n",
    "\n",
    "### Workflow Steps:\n",
    "1. **Select Working Directory** - Choose where to save annotations\n",
    "2. **Choose Container** - Select OMERO container (dataset, project, etc.)\n",
    "3. **Check Existing Tables** - Scan for existing annotation tables\n",
    "4. **Configure Parameters** - Set micro-SAM and processing parameters\n",
    "5. **Save Configuration** - Review and save your settings\n",
    "\n",
    "### Features:\n",
    "- Sequential workflow with progress tracking\n",
    "- Automatic table management and naming\n",
    "- Complete micro-SAM configuration\n",
    "- Configuration preview and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ OMERO Annotation Workflow Setup\n",
      "Follow the sequential workflow below:\n",
      "  1. Select working directory\n",
      "  2. Choose OMERO container\n",
      "  3. Check existing annotation tables\n",
      "  4. Configure micro-SAM parameters\n",
      "  5. Save configuration\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49764f123d4642f5848f7a1ea2d49ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>ğŸ”¬ OMERO Annotation Workflow</h3>', layout=Layout(margin='0 0 20px 0')), IntProgâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Next Step: Complete the workflow above, then run the cell below\n"
     ]
    }
   ],
   "source": [
    "# Create and display the workflow widget\n",
    "print(\"ğŸ”¬ OMERO Annotation Workflow Setup\")\n",
    "print(\"Follow the sequential workflow below:\")\n",
    "print(\"  1. Select working directory\")\n",
    "print(\"  2. Choose OMERO container\")\n",
    "print(\"  3. Check existing annotation tables\")\n",
    "print(\"  4. Configure micro-SAM parameters\")\n",
    "print(\"  5. Save configuration\")\n",
    "print()\n",
    "\n",
    "workflow_widget = create_workflow_widget(connection=conn)\n",
    "workflow_widget.display()\n",
    "\n",
    "print(\"\\nğŸ“ Next Step: Complete the workflow above, then run the cell below\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pipeline Execution\n",
    "\n",
    "Once you've completed the workflow configuration, we can run the complete annotation pipeline. This will:\n",
    "1. Create or resume from annotation tracking table\n",
    "2. Process images using micro-SAM\n",
    "3. Launch napari for interactive annotation\n",
    "4. Upload results to OMERO (or save locally in read-only mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration is valid!\n",
      "\n",
      "ğŸ“‹ Configuration Summary:\n",
      "   ğŸ“¦ Container: project (ID: 101)\n",
      "   ğŸ¯ Training Set: microsam_senescence_20250712_080635\n",
      "   ğŸ”¬ Model: vit_b_lm\n",
      "   ğŸ“º Channel: 0\n",
      "   ğŸ“ Output: /mnt/c/Users/Maarten/Documents/Github/omero_annotate_ai/examples/omero_annotations\n",
      "   ğŸ”„ Resume from Table: False\n",
      "   ğŸ“– Read-only Mode: False\n",
      "   ğŸ§© Patches: 1 per image (200Ã—200)\n",
      "\n",
      "ğŸ“Š Processing scope: 1 training + 1 validation\n"
     ]
    }
   ],
   "source": [
    "# Get configuration from workflow widget\n",
    "config = workflow_widget.get_config()\n",
    "\n",
    "# Validate configuration\n",
    "try:\n",
    "    config.validate()\n",
    "    print(\"âœ… Configuration is valid!\")\n",
    "except ValueError as e:\n",
    "    print(f\"âŒ Configuration validation failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Display configuration summary\n",
    "print(\"\\nğŸ“‹ Configuration Summary:\")\n",
    "print(f\"   ğŸ“¦ Container: {config.omero.container_type} (ID: {config.omero.container_id})\")\n",
    "print(f\"   ğŸ¯ Training Set: {config.training.trainingset_name}\")\n",
    "print(f\"   ğŸ”¬ Model: {config.microsam.model_type}\")\n",
    "print(f\"   ğŸ“º Channel: {config.omero.channel}\")\n",
    "print(f\"   ğŸ“ Output: {config.batch_processing.output_folder}\")\n",
    "print(f\"   ğŸ”„ Resume from Table: {config.workflow.resume_from_table}\")\n",
    "print(f\"   ğŸ“– Read-only Mode: {config.workflow.read_only_mode}\")\n",
    "\n",
    "if config.patches.use_patches:\n",
    "    print(f\"   ğŸ§© Patches: {config.patches.patches_per_image} per image ({config.patches.patch_size[0]}Ã—{config.patches.patch_size[1]})\")\n",
    "\n",
    "if config.microsam.three_d:\n",
    "    print(f\"   ğŸ§Š 3D processing: Enabled\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Processing scope: {'All images' if config.training.segment_all else f'{config.training.train_n} training + {config.training.validate_n} validation'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Validating project with ID 101...\n",
      "âœ… Found project: Senescence\n",
      "ğŸ“ Loading images from project 101\n",
      "ğŸ“Š Found 6 images\n",
      "\n",
      "ğŸ“Š Found 6 images to process\n",
      "\n",
      "ğŸ–¼ï¸ Sample images:\n",
      "   1. r01c06.tif (ID: 252)\n",
      "   2. r01c05.tif (ID: 255)\n",
      "   3. r01c02.tif (ID: 251)\n",
      "   4. r01c01.tif (ID: 256)\n",
      "   5. r01c04.tif (ID: 253)\n",
      "   ... and 1 more images\n",
      "\n",
      "âœ… Ready to process 6 images!\n"
     ]
    }
   ],
   "source": [
    "# Create pipeline and preview what will be processed\n",
    "pipeline = create_pipeline(config, conn)\n",
    "\n",
    "# Get container details\n",
    "container_type = config.omero.container_type\n",
    "container_id = config.omero.container_id\n",
    "\n",
    "print(f\"ğŸ” Validating {container_type} with ID {container_id}...\")\n",
    "\n",
    "# Validate container exists\n",
    "container = conn.getObject(container_type.capitalize(), container_id)\n",
    "if container is None:\n",
    "    raise ValueError(f\"{container_type.capitalize()} with ID {container_id} not found\")\n",
    "\n",
    "print(f\"âœ… Found {container_type}: {container.getName()}\")\n",
    "if container.getDescription():\n",
    "    print(f\"ğŸ“ Description: {container.getDescription()}\")\n",
    "\n",
    "# Get list of images that will be processed\n",
    "try:\n",
    "    images_list = pipeline.get_images_from_container()\n",
    "    print(f\"\\nğŸ“Š Found {len(images_list)} images to process\")\n",
    "    \n",
    "    # Show sample images\n",
    "    print(\"\\nğŸ–¼ï¸ Sample images:\")\n",
    "    for i, img in enumerate(images_list[:5]):\n",
    "        if hasattr(img, 'getName'):\n",
    "            print(f\"   {i+1}. {img.getName()} (ID: {img.getId()})\")\n",
    "        else:\n",
    "            img_obj = conn.getObject(\"Image\", img)\n",
    "            if img_obj:\n",
    "                print(f\"   {i+1}. {img_obj.getName()} (ID: {img})\")\n",
    "    \n",
    "    if len(images_list) > 5:\n",
    "        print(f\"   ... and {len(images_list) - 5} more images\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error getting images from container: {e}\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\nâœ… Ready to process {len(images_list)} images!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting annotation pipeline...\n",
      "   ğŸ“Š Processing 6 images\n",
      "   ğŸ”¬ Using micro-SAM model: vit_b_lm\n",
      "   âš¡ Processing: All images in one batch\n",
      "   ğŸ¨ Napari will open for interactive annotation\n",
      "   ğŸ“ Close napari windows when annotation is complete\n",
      "\n",
      "ğŸ“ Loading images from project 101\n",
      "ğŸ“Š Found 6 images\n",
      "ğŸš€ Starting micro-SAM annotation pipeline\n",
      "ğŸ“Š Processing 6 images with model: vit_b_lm\n",
      "ğŸ“‹ Creating new tracking table: microsam_training_microsam_senescence_20250712_080635\n",
      "ğŸ“‹ ROI namespace: omero_annotate_ai.table.microsam_training_microsam_senescence_20250712_080635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maarten/miniconda3/envs/micro-sam/lib/python3.12/site-packages/omero_annotate_ai/omero/omero_functions.py:105: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna(False).infer_objects(copy=False).astype(bool)\n",
      "/home/maarten/miniconda3/envs/micro-sam/lib/python3.12/site-packages/omero_annotate_ai/omero/omero_functions.py:105: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna(False).infer_objects(copy=False).astype(bool)\n",
      "/home/maarten/miniconda3/envs/micro-sam/lib/python3.12/site-packages/omero_annotate_ai/omero/omero_functions.py:105: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna(False).infer_objects(copy=False).astype(bool)\n",
      "/home/maarten/miniconda3/envs/micro-sam/lib/python3.12/site-packages/omero_annotate_ai/omero/omero_functions.py:105: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna(False).infer_objects(copy=False).astype(bool)\n",
      "/home/maarten/miniconda3/envs/micro-sam/lib/python3.12/site-packages/omero_annotate_ai/omero/omero_functions.py:105: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna(False).infer_objects(copy=False).astype(bool)\n",
      "/home/maarten/miniconda3/envs/micro-sam/lib/python3.12/site-packages/omero_annotate_ai/omero/omero_functions.py:110: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna('None').infer_objects(copy=False).astype(str)\n",
      "/home/maarten/miniconda3/envs/micro-sam/lib/python3.12/site-packages/omero_annotate_ai/omero/omero_functions.py:110: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna('None').infer_objects(copy=False).astype(str)\n",
      "/home/maarten/miniconda3/envs/micro-sam/lib/python3.12/site-packages/omero_annotate_ai/omero/omero_functions.py:110: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna('None').infer_objects(copy=False).astype(str)\n",
      "/home/maarten/miniconda3/envs/micro-sam/lib/python3.12/site-packages/omero_annotate_ai/omero/omero_functions.py:110: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna('None').infer_objects(copy=False).astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Created tracking table 'microsam_training_microsam_senescence_20250712_080635' with 2 units\n",
      "   Container: project 101\n",
      "   Table ID: 753\n",
      "object group 0\n",
      "Stored configuration as annotation ID: 754\n",
      "ğŸ“‹ Getting unprocessed units from table 753\n",
      "ğŸ“‹ Found 2 unprocessed units\n",
      "ğŸ“‹ Found 2 processing units\n",
      "ğŸ”„ Processing batch 1/1\n",
      "ğŸ“Š Loading 1 images using dask...\n",
      "ğŸ’¾ Materializing dask arrays to numpy...\n",
      "   Processing chunk 1/1\n",
      "âœ… Successfully loaded 1 images\n",
      "ğŸ“Š Loading 1 images using dask...\n",
      "ğŸ’¾ Materializing dask arrays to numpy...\n",
      "   Processing chunk 1/1\n",
      "âœ… Successfully loaded 1 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precompute state for files:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:31<00:31, 31.09s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# Run the complete workflow - this is the key call!\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     table_id, processed_images = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_full_workflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mğŸ‰ Annotation pipeline completed successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mğŸ“Š Processed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(processed_images)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m images\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/omero_annotate_ai/core/pipeline.py:582\u001b[39m, in \u001b[36mAnnotationPipeline.run_full_workflow\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    579\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m images_list:\n\u001b[32m    580\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo images found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.config.omero.container_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.config.omero.container_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m582\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/omero_annotate_ai/core/pipeline.py:518\u001b[39m, in \u001b[36mAnnotationPipeline.run\u001b[39m\u001b[34m(self, images_list)\u001b[39m\n\u001b[32m    515\u001b[39m     batch_with_images.append((image_obj, seq_val, meta, row_idx))\n\u001b[32m    517\u001b[39m \u001b[38;5;66;03m# Run annotation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m518\u001b[39m annotation_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_microsam_annotation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_with_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[38;5;66;03m# Process results\u001b[39;00m\n\u001b[32m    521\u001b[39m table_id = \u001b[38;5;28mself\u001b[39m._process_annotation_results(annotation_results, table_id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/omero_annotate_ai/core/pipeline.py:319\u001b[39m, in \u001b[36mAnnotationPipeline._run_microsam_annotation\u001b[39m\u001b[34m(self, batch_data)\u001b[39m\n\u001b[32m    316\u001b[39m napari_settings.application.save_window_geometry = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    318\u001b[39m \u001b[38;5;66;03m# Run image series annotator\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m segmentation_results = \u001b[43mimage_series_annotator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43membedding_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_volumetric\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmicrosam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthree_d\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    328\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mresults\u001b[39m\u001b[33m\"\u001b[39m: segmentation_results,\n\u001b[32m    329\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    330\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mzarr_path\u001b[39m\u001b[33m\"\u001b[39m: zarr_path,\n\u001b[32m    331\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33membedding_path\u001b[39m\u001b[33m\"\u001b[39m: embedding_path\n\u001b[32m    332\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/micro_sam/sam_annotator/image_series_annotator.py:180\u001b[39m, in \u001b[36mimage_series_annotator\u001b[39m\u001b[34m(images, output_folder, model_type, embedding_path, tile_shape, halo, viewer, return_viewer, precompute_amg_state, checkpoint_path, is_volumetric, device, prefer_decoder, skip_segmented)\u001b[39m\n\u001b[32m    177\u001b[39m os.makedirs(output_folder, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m# Precompute embeddings and amg state (if corresponding options set).\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m predictor, decoder, embedding_paths = \u001b[43m_precompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhalo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecompute_amg_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43mndim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_volumetric\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer_decoder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefer_decoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    187\u001b[39m next_image_id = \u001b[32m0\u001b[39m\n\u001b[32m    188\u001b[39m have_inputs_as_arrays = \u001b[38;5;28misinstance\u001b[39m(images[next_image_id], np.ndarray)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/micro_sam/sam_annotator/image_series_annotator.py:46\u001b[39m, in \u001b[36m_precompute\u001b[39m\u001b[34m(images, model_type, embedding_path, tile_shape, halo, precompute_amg_state, checkpoint_path, device, ndim, prefer_decoder)\u001b[39m\n\u001b[32m     44\u001b[39m     embedding_paths = [\u001b[38;5;28;01mNone\u001b[39;00m] * \u001b[38;5;28mlen\u001b[39m(images)\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[43m_precompute_state_for_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mndim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtile_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhalo\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhalo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprecompute_amg_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprecompute_amg_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(images[\u001b[32m0\u001b[39m], np.ndarray):\n\u001b[32m     51\u001b[39m         embedding_paths = [\n\u001b[32m     52\u001b[39m             os.path.join(embedding_path, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33membedding_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m05\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.zarr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i, path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(images)\n\u001b[32m     53\u001b[39m         ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/micro_sam/precompute_state.py:213\u001b[39m, in \u001b[36m_precompute_state_for_files\u001b[39m\u001b[34m(predictor, input_files, output_path, key, ndim, tile_shape, halo, precompute_amg_state, decoder)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    211\u001b[39m     out_path = os.path.join(output_path, os.path.basename(file_path))\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m \u001b[43m_precompute_state_for_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mndim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtile_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhalo\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhalo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprecompute_amg_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprecompute_amg_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m idx += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/micro_sam/precompute_state.py:166\u001b[39m, in \u001b[36m_precompute_state_for_file\u001b[39m\u001b[34m(predictor, input_path, output_path, key, ndim, tile_shape, halo, precompute_amg_state, decoder, verbose)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# Precompute the image embeddings.\u001b[39;00m\n\u001b[32m    165\u001b[39m output_path = Path(output_path).with_suffix(\u001b[33m\"\u001b[39m\u001b[33m.zarr\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m embeddings = \u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprecompute_image_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mndim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtile_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhalo\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhalo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[38;5;66;03m# Precompute the state for automatic instance segmnetaiton (AMG or AIS).\u001b[39;00m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m precompute_amg_state:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/micro_sam/util.py:1064\u001b[39m, in \u001b[36mprecompute_image_embeddings\u001b[39m\u001b[34m(predictor, input_, save_path, lazy_loading, ndim, tile_shape, halo, verbose, batch_size, pbar_init, pbar_update)\u001b[39m\n\u001b[32m   1061\u001b[39m _, pbar_init, pbar_update, pbar_close = handle_pbar(verbose, pbar_init, pbar_update)\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ndim == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m tile_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m     embeddings = \u001b[43m_compute_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbar_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbar_update\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m ndim == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m tile_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1066\u001b[39m     embeddings = _compute_tiled_2d(input_, predictor, tile_shape, halo, f, pbar_init, pbar_update, batch_size)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/micro_sam/util.py:791\u001b[39m, in \u001b[36m_compute_2d\u001b[39m\u001b[34m(input_, predictor, f, save_path, pbar_init, pbar_update)\u001b[39m\n\u001b[32m    789\u001b[39m \u001b[38;5;66;03m# Otherwise we have to compute the embeddings.\u001b[39;00m\n\u001b[32m    790\u001b[39m predictor.reset_image()\n\u001b[32m--> \u001b[39m\u001b[32m791\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    792\u001b[39m features = predictor.get_image_embedding().cpu().numpy()\n\u001b[32m    793\u001b[39m original_size = predictor.original_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/segment_anything/predictor.py:60\u001b[39m, in \u001b[36mSamPredictor.set_image\u001b[39m\u001b[34m(self, image, image_format)\u001b[39m\n\u001b[32m     57\u001b[39m input_image_torch = torch.as_tensor(input_image, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     58\u001b[39m input_image_torch = input_image_torch.permute(\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m).contiguous()[\u001b[38;5;28;01mNone\u001b[39;00m, :, :, :]\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mset_torch_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/segment_anything/predictor.py:89\u001b[39m, in \u001b[36mSamPredictor.set_torch_image\u001b[39m\u001b[34m(self, transformed_image, original_image_size)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28mself\u001b[39m.input_size = \u001b[38;5;28mtuple\u001b[39m(transformed_image.shape[-\u001b[32m2\u001b[39m:])\n\u001b[32m     88\u001b[39m input_image = \u001b[38;5;28mself\u001b[39m.model.preprocess(transformed_image)\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[38;5;28mself\u001b[39m.features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;28mself\u001b[39m.is_image_set = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/segment_anything/modeling/image_encoder.py:112\u001b[39m, in \u001b[36mImageEncoderViT.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    109\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.pos_embed\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     x = \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m x = \u001b[38;5;28mself\u001b[39m.neck(x.permute(\u001b[32m0\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m))\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/segment_anything/modeling/image_encoder.py:174\u001b[39m, in \u001b[36mBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    171\u001b[39m     H, W = x.shape[\u001b[32m1\u001b[39m], x.shape[\u001b[32m2\u001b[39m]\n\u001b[32m    172\u001b[39m     x, pad_hw = window_partition(x, \u001b[38;5;28mself\u001b[39m.window_size)\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[38;5;66;03m# Reverse window partition\u001b[39;00m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.window_size > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/segment_anything/modeling/image_encoder.py:234\u001b[39m, in \u001b[36mAttention.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    231\u001b[39m attn = (q * \u001b[38;5;28mself\u001b[39m.scale) @ k.transpose(-\u001b[32m2\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_rel_pos:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m     attn = \u001b[43madd_decomposed_rel_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrel_pos_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrel_pos_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m attn = attn.softmax(dim=-\u001b[32m1\u001b[39m)\n\u001b[32m    237\u001b[39m x = (attn @ v).view(B, \u001b[38;5;28mself\u001b[39m.num_heads, H, W, -\u001b[32m1\u001b[39m).permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m4\u001b[39m).reshape(B, H, W, -\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/segment_anything/modeling/image_encoder.py:355\u001b[39m, in \u001b[36madd_decomposed_rel_pos\u001b[39m\u001b[34m(attn, q, rel_pos_h, rel_pos_w, q_size, k_size)\u001b[39m\n\u001b[32m    353\u001b[39m r_q = q.reshape(B, q_h, q_w, dim)\n\u001b[32m    354\u001b[39m rel_h = torch.einsum(\u001b[33m\"\u001b[39m\u001b[33mbhwc,hkc->bhwk\u001b[39m\u001b[33m\"\u001b[39m, r_q, Rh)\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m rel_w = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbhwc,wkc->bhwk\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    357\u001b[39m attn = (\n\u001b[32m    358\u001b[39m     attn.view(B, q_h, q_w, k_h, k_w) + rel_h[:, :, :, :, \u001b[38;5;28;01mNone\u001b[39;00m] + rel_w[:, :, :, \u001b[38;5;28;01mNone\u001b[39;00m, :]\n\u001b[32m    359\u001b[39m ).view(B, q_h * q_w, k_h * k_w)\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attn\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/torch/functional.py:407\u001b[39m, in \u001b[36meinsum\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, *_operands)\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) <= \u001b[32m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum.enabled:\n\u001b[32m    405\u001b[39m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[32m    406\u001b[39m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    409\u001b[39m path = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum.is_available():\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-! 07/12/25 09:46:54.263 --Ice.Config=: warning: Proxy keep alive failed.\n"
     ]
    }
   ],
   "source": [
    "# Run the complete annotation pipeline\n",
    "print(\"ğŸš€ Starting annotation pipeline...\")\n",
    "print(f\"   ğŸ“Š Processing {len(images_list)} images\")\n",
    "print(f\"   ğŸ”¬ Using micro-SAM model: {config.microsam.model_type}\")\n",
    "\n",
    "if config.batch_processing.batch_size == 0:\n",
    "    print(f\"   âš¡ Processing: All images in one batch\")\n",
    "else:\n",
    "    print(f\"   ğŸ“¦ Processing: Batches of {config.batch_processing.batch_size} images\")\n",
    "\n",
    "print(f\"   ğŸ¨ Napari will open for interactive annotation\")\n",
    "print(f\"   ğŸ“ Close napari windows when annotation is complete\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    # Run the complete workflow - this is the key call!\n",
    "    table_id, processed_images = pipeline.run_full_workflow()\n",
    "    \n",
    "    print(f\"\\nğŸ‰ Annotation pipeline completed successfully!\")\n",
    "    print(f\"ğŸ“Š Processed {len(processed_images)} images\")\n",
    "    print(f\"ğŸ“‹ Tracking table ID: {table_id}\")\n",
    "    \n",
    "    if config.workflow.read_only_mode:\n",
    "        print(f\"ğŸ’¾ Annotations saved locally to: {config.batch_processing.output_folder}\")\n",
    "    else:\n",
    "        print(f\"â˜ï¸ Annotations uploaded to OMERO\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error during annotation pipeline: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results and Export\n",
    "\n",
    "Review the results and export your configuration for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results summary\n",
    "print(\"ğŸ“Š Pipeline Results Summary:\")\n",
    "print(f\"   ğŸ¯ Training Set: {config.training.trainingset_name}\")\n",
    "print(f\"   ğŸ“‹ Tracking Table ID: {table_id}\")\n",
    "print(f\"   ğŸ“Š Images Processed: {len(processed_images)}\")\n",
    "print(f\"   ğŸ“¦ Container: {config.omero.container_type} (ID: {config.omero.container_id})\")\n",
    "print(f\"   ğŸ”¬ Model Used: {config.microsam.model_type}\")\n",
    "print(f\"   ğŸ“ Output Location: {config.batch_processing.output_folder}\")\n",
    "\n",
    "# Show processed images\n",
    "if processed_images:\n",
    "    print(f\"\\nğŸ–¼ï¸ Processed Images:\")\n",
    "    for i, img_id in enumerate(processed_images[:10]):\n",
    "        img_obj = conn.getObject(\"Image\", img_id)\n",
    "        if img_obj:\n",
    "            print(f\"   {i+1}. {img_obj.getName()} (ID: {img_id})\")\n",
    "    \n",
    "    if len(processed_images) > 10:\n",
    "        print(f\"   ... and {len(processed_images) - 10} more images\")\n",
    "\n",
    "print(f\"\\nâœ… Pipeline completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export configuration for future use\n",
    "config_filename = f\"annotation_config_{config.training.trainingset_name}.yaml\"\n",
    "config_path = Path(config.batch_processing.output_folder) / config_filename\n",
    "\n",
    "try:\n",
    "    config.save_yaml(config_path)\n",
    "    print(f\"ğŸ’¾ Configuration saved to: {config_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Could not save to output folder, saving to current directory\")\n",
    "    config.save_yaml(config_filename)\n",
    "    print(f\"ğŸ’¾ Configuration saved to: {config_filename}\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ Configuration Summary:\")\n",
    "print(config.get_workflow_summary())\n",
    "\n",
    "print(f\"\\nğŸ”„ To reuse this configuration:\")\n",
    "print(f\"```python\")\n",
    "print(f\"from omero_annotate_ai import load_config\")\n",
    "print(f\"config = load_config('{config_filename}')\")\n",
    "print(f\"```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cleanup\n",
    "\n",
    "Close the OMERO connection when finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close OMERO connection\n",
    "if 'conn' in locals() and conn is not None:\n",
    "    conn.close()\n",
    "    print(\"ğŸ”Œ OMERO connection closed\")\n",
    "\n",
    "print(f\"\\nğŸ‰ Streamlined workflow completed!\")\n",
    "print(f\"ğŸ“Š Total images processed: {len(processed_images) if 'processed_images' in locals() else 0}\")\n",
    "print(f\"ğŸ’¾ Configuration saved for future use\")\n",
    "print(f\"âœ¨ Ready for next annotation workflow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### ğŸ”„ Reusing Configurations\n",
    "Save time by reusing your configuration:\n",
    "```python\n",
    "from omero_annotate_ai import load_config, create_pipeline\n",
    "config = load_config('annotation_config_your_training_set.yaml')\n",
    "pipeline = create_pipeline(config, conn)\n",
    "```\n",
    "\n",
    "### ğŸ§ª Advanced Usage\n",
    "- **Batch Processing**: Set `batch_size > 0` for large datasets\n",
    "- **3D Processing**: Enable `three_d` for volumetric data\n",
    "- **Patch Processing**: Use `use_patches` for large images\n",
    "- **Read-only Mode**: Use `read_only_mode` for testing\n",
    "\n",
    "### ğŸ“š Documentation\n",
    "- **Package Documentation**: See `CLAUDE.md` for development guidelines\n",
    "- **API Reference**: Use `help(function_name)` for detailed documentation\n",
    "- **Examples**: Check the `examples/` folder for more use cases\n",
    "\n",
    "### ğŸ› ï¸ Development\n",
    "- **Testing**: Use `make test` to run the test suite\n",
    "- **Linting**: Use `make lint` for code quality checks\n",
    "- **Documentation**: Use `make docs` to build documentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micro-sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
