{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Micro-SAM Training from OMERO Data\n",
    "\n",
    "Train micro-SAM models using annotation tables from OMERO with automated data preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available: CPU\n"
     ]
    }
   ],
   "source": [
    "# Import the package with training convenience functions\n",
    "from omero_annotate_ai import (\n",
    "    create_omero_connection_widget,\n",
    "    create_training_data_widget,\n",
    "    prepare_training_data_from_table,\n",
    "    setup_training,    # Convenience function for training setup\n",
    "    run_training       # Convenience function for training execution\n",
    ")\n",
    "\n",
    "# Additional imports\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "print(f\"Device available: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. OMERO Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration from connection history: root@localhost\n",
      "Password loaded from keychain\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64af315f27846d7ba045c6051ed67ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"\\n                <h3>ðŸ”Œ OMERO Server Connection</h3>\\n                <div style='fâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create and display OMERO connection widget\n",
    "conn_widget = create_omero_connection_widget()\n",
    "conn_widget.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to OMERO as: root\n"
     ]
    }
   ],
   "source": [
    "# Get the OMERO connection\n",
    "conn = conn_widget.get_connection()\n",
    "\n",
    "if conn is None:\n",
    "    raise ConnectionError(\"No OMERO connection established.\")\n",
    "\n",
    "print(f\"Connected to OMERO as: {conn.getUser().getName()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698afcc0fa0a44d0be640b4910e17c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>ðŸŽ¯ Training Data Selection</h3>', layout=Layout(margin='0 0 20px 0')), HTML(valuâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create training data selection widget\n",
    "training_widget = create_training_data_widget(connection=conn)\n",
    "training_widget.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected training table:\n",
      "  Table ID: 2107\n",
      "  Table Name: testscreen_20260218_183216\n",
      "  Created: Unknown\n"
     ]
    }
   ],
   "source": [
    "# Get selected training table\n",
    "selected_table_id = training_widget.get_selected_table_id()\n",
    "selected_table_info = training_widget.get_selected_table_info()\n",
    "\n",
    "if selected_table_id:\n",
    "    print(f\"Selected training table:\")\n",
    "    print(f\"  Table ID: {selected_table_id}\")\n",
    "    print(f\"  Table Name: {selected_table_info.get('name', 'Unknown')}\")\n",
    "    print(f\"  Created: {selected_table_info.get('created', 'Unknown')}\")\n",
    "else:\n",
    "    raise ValueError(\"No training table selected. Please select a table above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setup Training Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training output directory: /var/home/maartenpaul/omero-annotate-ai/micro-sam_models/micro-sam-20260218_194501\n"
     ]
    }
   ],
   "source": [
    "# Create output directory for training\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "home_dir = Path.home()\n",
    "models_dir = home_dir / \"omero-annotate-ai/micro-sam_models\"\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "folder_name = f\"micro-sam-{timestamp}\"\n",
    "output_directory = models_dir / folder_name\n",
    "output_directory.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Training output directory: {output_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Automated Data Preparation\n",
    "\n",
    "Use the automated data preparation function to download and organize training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:omero_annotate_ai.training:Starting training data preparation from table 2107\n",
      "DEBUG:omero_annotate_ai.training:Parameters: output_dir=/var/home/maartenpaul/omero-annotate-ai/micro-sam_models/micro-sam-20260218_194501, validation_split=0.2, clean_existing=True\n",
      "INFO:omero_annotate_ai.training:Loaded table with 5 rows\n",
      "INFO:omero_annotate_ai.training:Table saved to: /var/home/maartenpaul/omero-annotate-ai/micro-sam_models/micro-sam-20260218_194501/table_2107.csv\n",
      "INFO:omero_annotate_ai.training:Using 5 processed rows for training\n",
      "DEBUG:omero_annotate_ai.training:Optional columns found: ['is_volumetric']\n",
      "INFO:omero_annotate_ai.training:Table schema validated for processing\n",
      "INFO:omero_annotate_ai.training:Using existing train/validate split from table\n",
      "INFO:omero_annotate_ai.training:Using 3 training images and 2 validation images\n",
      "INFO:omero_annotate_ai.training:Preparing training dataset: 3 items to process\n",
      "Preparing training data:   0%|          | 0/3 [00:00<?, ?it/s]DEBUG:omero_annotate_ai.training:Item 0 - Image ID: 287, Patch: True, Dimensions: 50x50 at (0,0), Volumetric: False\n",
      "DEBUG:omero_annotate_ai.training:2D Patch Request - start_coords: (0, 0, 0, 0, 0), dimensions: 50x50\n",
      "DEBUG:omero_annotate_ai.training:Returned array shape: (50, 50, 1, 1, 1)\n",
      "DEBUG:omero_annotate_ai.training:Extracted 2D shape: (50, 50)\n",
      "DEBUG:omero_annotate_ai.training:Saved 2D TIFF to /var/home/maartenpaul/omero-annotate-ai/micro-sam_models/micro-sam-20260218_194501/training_input/input_00000.tif with shape (50, 50)\n",
      "DEBUG:omero_annotate_ai.training:Attempting to download label with ID: 2102\n",
      "DEBUG:omero_annotate_ai.training:File annotation found: seg_00000.tif\n",
      "DEBUG:omero_annotate_ai.training:Label shape: (50, 50) saved to /var/home/maartenpaul/omero-annotate-ai/micro-sam_models/micro-sam-20260218_194501/training_label/label_00000.tif\n",
      "Preparing training data:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  4.83it/s]DEBUG:omero_annotate_ai.training:Item 1 - Image ID: 144, Patch: True, Dimensions: 50x50 at (0,0), Volumetric: False\n",
      "DEBUG:omero_annotate_ai.training:2D Patch Request - start_coords: (0, 0, 0, 0, 0), dimensions: 50x50\n",
      "DEBUG:omero_annotate_ai.training:Returned array shape: (50, 50, 1, 1, 1)\n",
      "DEBUG:omero_annotate_ai.training:Extracted 2D shape: (50, 50)\n",
      "DEBUG:omero_annotate_ai.training:Saved 2D TIFF to /var/home/maartenpaul/omero-annotate-ai/micro-sam_models/micro-sam-20260218_194501/training_input/input_00001.tif with shape (50, 50)\n",
      "DEBUG:omero_annotate_ai.training:Attempting to download label with ID: 2103\n",
      "DEBUG:omero_annotate_ai.training:File annotation found: seg_00001.tif\n",
      "DEBUG:omero_annotate_ai.training:Label shape: (50, 50) saved to /var/home/maartenpaul/omero-annotate-ai/micro-sam_models/micro-sam-20260218_194501/training_label/label_00001.tif\n",
      "Preparing training data:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  5.76it/s]DEBUG:omero_annotate_ai.training:Item 2 - Image ID: 124, Patch: True, Dimensions: 50x50 at (0,0), Volumetric: False\n",
      "DEBUG:omero_annotate_ai.training:2D Patch Request - start_coords: (0, 0, 0, 0, 0), dimensions: 50x50\n",
      "DEBUG:omero_annotate_ai.training:Returned array shape: (50, 50, 1, 1, 1)\n",
      "DEBUG:omero_annotate_ai.training:Extracted 2D shape: (50, 50)\n",
      "DEBUG:omero_annotate_ai.training:Saved 2D TIFF to /var/home/maartenpaul/omero-annotate-ai/micro-sam_models/micro-sam-20260218_194501/training_input/input_00002.tif with shape (50, 50)\n",
      "DEBUG:omero_annotate_ai.training:Attempting to download label with ID: 2104\n",
      "DEBUG:omero_annotate_ai.training:File annotation found: seg_00002.tif\n",
      "DEBUG:omero_annotate_ai.training:Label shape: (50, 50) saved to /var/home/maartenpaul/omero-annotate-ai/micro-sam_models/micro-sam-20260218_194501/training_label/label_00002.tif\n",
      "Preparing training data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.16it/s]\n",
      "INFO:omero_annotate_ai.training:Preparing val dataset: 2 items to process\n",
      "Preparing val data:   0%|          | 0/2 [00:00<?, ?it/s]DEBUG:omero_annotate_ai.training:Item 0 - Image ID: 135, Patch: True, Dimensions: 50x50 at (0,0), Volumetric: False\n",
      "DEBUG:omero_annotate_ai.training:2D Patch Request - start_coords: (0, 0, 0, 0, 0), dimensions: 50x50\n",
      "DEBUG:omero_annotate_ai.training:Returned array shape: (50, 50, 1, 1, 1)\n",
      "DEBUG:omero_annotate_ai.training:Extracted 2D shape: (50, 50)\n",
      "DEBUG:omero_annotate_ai.training:Saved 2D TIFF to /var/home/maartenpaul/omero-annotate-ai/micro-sam_models/micro-sam-20260218_194501/val_input/input_00000.tif with shape (50, 50)\n",
      "DEBUG:omero_annotate_ai.training:Attempting to download label with ID: 2105\n",
      "DEBUG:omero_annotate_ai.training:File annotation found: seg_00003.tif\n",
      "DEBUG:omero_annotate_ai.training:Label shape: (50, 50) saved to /var/home/maartenpaul/omero-annotate-ai/micro-sam_models/micro-sam-20260218_194501/val_label/label_00000.tif\n",
      "Preparing val data:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  6.97it/s]DEBUG:omero_annotate_ai.training:Item 1 - Image ID: 251, Patch: True, Dimensions: 50x50 at (0,0), Volumetric: False\n",
      "DEBUG:omero_annotate_ai.training:2D Patch Request - start_coords: (0, 0, 0, 0, 0), dimensions: 50x50\n",
      "DEBUG:omero_annotate_ai.training:Returned array shape: (50, 50, 1, 1, 1)\n",
      "DEBUG:omero_annotate_ai.training:Extracted 2D shape: (50, 50)\n",
      "DEBUG:omero_annotate_ai.training:Saved 2D TIFF to /var/home/maartenpaul/omero-annotate-ai/micro-sam_models/micro-sam-20260218_194501/val_input/input_00001.tif with shape (50, 50)\n",
      "DEBUG:omero_annotate_ai.training:Attempting to download label with ID: 2106\n",
      "DEBUG:omero_annotate_ai.training:File annotation found: seg_00004.tif\n",
      "DEBUG:omero_annotate_ai.training:Label shape: (50, 50) saved to /var/home/maartenpaul/omero-annotate-ai/micro-sam_models/micro-sam-20260218_194501/val_label/label_00001.tif\n",
      "Preparing val data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.01it/s]\n",
      "INFO:omero_annotate_ai.training:Preparing val dataset: 2 items to process\n",
      "Preparing val data:   0%|          | 0/2 [00:00<?, ?it/s]DEBUG:omero_annotate_ai.training:Item 0 - Image ID: 135, Patch: True, Dimensions: 50x50 at (0,0), Volumetric: False\n",
      "DEBUG:omero_annotate_ai.training:2D Patch Request - start_coords: (0, 0, 0, 0, 0), dimensions: 50x50\n",
      "DEBUG:omero_annotate_ai.training:Returned array shape: (50, 50, 1, 1, 1)\n",
      "DEBUG:omero_annotate_ai.training:Extracted 2D shape: (50, 50)\n",
      "DEBUG:omero_annotate_ai.training:Saved 2D TIFF to /var/home/maartenpaul/omero-annotate-ai/micro-sam_models/micro-sam-20260218_194501/val_input/input_00000.tif with shape (50, 50)\n",
      "DEBUG:omero_annotate_ai.training:Attempting to download label with ID: 2105\n",
      "DEBUG:omero_annotate_ai.training:File annotation found: seg_00003.tif\n",
      "DEBUG:omero_annotate_ai.training:Label shape: (50, 50) saved to /var/home/maartenpaul/omero-annotate-ai/micro-sam_models/micro-sam-20260218_194501/val_label/label_00000.tif\n",
      "Preparing val data:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  7.10it/s]DEBUG:omero_annotate_ai.training:Item 1 - Image ID: 251, Patch: True, Dimensions: 50x50 at (0,0), Volumetric: False\n",
      "DEBUG:omero_annotate_ai.training:2D Patch Request - start_coords: (0, 0, 0, 0, 0), dimensions: 50x50\n",
      "DEBUG:omero_annotate_ai.training:Returned array shape: (50, 50, 1, 1, 1)\n",
      "DEBUG:omero_annotate_ai.training:Extracted 2D shape: (50, 50)\n",
      "DEBUG:omero_annotate_ai.training:Saved 2D TIFF to /var/home/maartenpaul/omero-annotate-ai/micro-sam_models/micro-sam-20260218_194501/val_input/input_00001.tif with shape (50, 50)\n",
      "DEBUG:omero_annotate_ai.training:Attempting to download label with ID: 2106\n",
      "DEBUG:omero_annotate_ai.training:File annotation found: seg_00004.tif\n",
      "DEBUG:omero_annotate_ai.training:Label shape: (50, 50) saved to /var/home/maartenpaul/omero-annotate-ai/micro-sam_models/micro-sam-20260218_194501/val_label/label_00001.tif\n",
      "Preparing val data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.22it/s]\n",
      "DEBUG:omero_annotate_ai.training:Cleaned up temporary directory: /var/home/maartenpaul/omero-annotate-ai/micro-sam_models/micro-sam-20260218_194501/tmp\n",
      "INFO:omero_annotate_ai.training:Training data prepared successfully in: /var/home/maartenpaul/omero-annotate-ai/micro-sam_models/micro-sam-20260218_194501\n",
      "INFO:omero_annotate_ai.training:Statistics: {'n_training_images': 3, 'n_training_labels': 3, 'n_val_images': 2, 'n_val_labels': 2, 'total_rows_processed': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training data preparation completed successfully!\n",
      "\n",
      "Dataset statistics:\n",
      "  n_training_images: 3\n",
      "  n_training_labels: 3\n",
      "  n_val_images: 2\n",
      "  n_val_labels: 2\n",
      "  total_rows_processed: 5\n",
      "\n",
      "Directory structure created:\n",
      "  Training images: /var/home/maartenpaul/omero-annotate-ai/micro-sam_models/micro-sam-20260218_194501/training_input\n",
      "  Training labels: /var/home/maartenpaul/omero-annotate-ai/micro-sam_models/micro-sam-20260218_194501/training_label\n",
      "  Validation images: /var/home/maartenpaul/omero-annotate-ai/micro-sam_models/micro-sam-20260218_194501/val_input\n",
      "  Validation labels: /var/home/maartenpaul/omero-annotate-ai/micro-sam_models/micro-sam-20260218_194501/val_label\n"
     ]
    }
   ],
   "source": [
    "# Run automated data preparation\n",
    "try:\n",
    "    training_result = prepare_training_data_from_table(\n",
    "        conn=conn,\n",
    "        table_id=selected_table_id,\n",
    "        training_name= selected_table_info.get('name', f\"training_table_{selected_table_id}\"),\n",
    "        output_dir=output_directory,\n",
    "        clean_existing=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining data preparation completed successfully!\")\n",
    "    print(f\"\\nDataset statistics:\")\n",
    "    for key, value in training_result['stats'].items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Store paths for later use in training\n",
    "    training_input_dir = training_result['training_input']\n",
    "    training_label_dir = training_result['training_label']\n",
    "    val_input_dir = training_result['val_input']\n",
    "    val_label_dir = training_result['val_label']\n",
    "    \n",
    "    print(f\"\\nDirectory structure created:\")\n",
    "    print(f\"  Training images: {training_input_dir}\")\n",
    "    print(f\"  Training labels: {training_label_dir}\")\n",
    "    print(f\"  Validation images: {val_input_dir}\")\n",
    "    print(f\"  Validation labels: {val_label_dir}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during data preparation: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Micro-SAM Training Setup\n",
    "\n",
    "Configure and run micro-SAM training using the prepared data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration prepared!\n",
      "Model name: testscreen_20260218_183216_20260218_194501\n",
      "Output directory: /var/home/maartenpaul/omero-annotate-ai/micro-sam_models/micro-sam-20260218_194501\n",
      "Training epochs: 10\n",
      "Calculated iterations: 30\n"
     ]
    }
   ],
   "source": [
    "# âœ¨ Setup training configuration using convenience function\n",
    "training_config = setup_training(\n",
    "    training_result,\n",
    "    model_name=f\"{selected_table_info.get('name', 'micro_sam_training')}_{timestamp}\",\n",
    "    epochs=10,               # Primary parameter: number of epochs (use 50+ for real training)\n",
    "    batch_size=1,            # Adjust based on GPU memory\n",
    "    learning_rate=1e-5,      # Conservative learning rate\n",
    "    patch_shape=(512, 512),  # Input patch size\n",
    "    model_type=\"vit_b_lm\",       # SAM model variant\n",
    "    n_objects_per_batch=25   # Objects per batch for sampling\n",
    ")\n",
    "\n",
    "print(\"Training configuration prepared!\")\n",
    "print(f'Model name: {training_config[\"model_name\"]}')\n",
    "print(f'Output directory: {training_config[\"output_dir\"]}')\n",
    "print(f'Training epochs: {training_config[\"epochs\"]}')\n",
    "print(f'Calculated iterations: {training_config[\"n_iterations\"]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting micro-SAM training...\n",
      "Starting micro-SAM training...\n",
      "Model name: testscreen_20260218_183216_20260218_194501\n",
      "Model type: vit_b_lm\n",
      "Training configuration:\n",
      "  Patch shape: (512, 512)\n",
      "  Batch size: 1\n",
      "  Learning rate: 1e-05\n",
      "  Epochs: 10\n",
      "  Objects per batch: 25\n",
      "  Checkpoint folder: /var/home/maartenpaul/omero-annotate-ai/micro-sam_models/micro-sam-20260218_194501/checkpoints\n",
      "  Using patch shape: (1, 512, 512)\n",
      "Training device: cpu\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "torch_em.segmentation.default_segmentation_loader() got multiple values for keyword argument 'is_seg_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# âœ¨ Execute training with convenience function\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting micro-SAM training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m training_results = \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmicrosam\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mðŸŽ‰ Training completed successfully!\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mTraining Results:\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/omero_annotate_ai/src/omero_annotate_ai/processing/training_utils.py:146\u001b[39m, in \u001b[36mrun_training\u001b[39m\u001b[34m(training_config, framework)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03mExecute training with framework-specific implementation.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    143\u001b[39m \u001b[33;03m    ImportError: If required framework packages are not available\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m framework.lower() == \u001b[33m\"\u001b[39m\u001b[33mmicrosam\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_run_microsam_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    148\u001b[39m     supported_frameworks = [\u001b[33m\"\u001b[39m\u001b[33mmicrosam\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/omero_annotate_ai/src/omero_annotate_ai/processing/training_utils.py:213\u001b[39m, in \u001b[36m_run_microsam_training\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# Create data loaders with correct API\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m train_loader = \u001b[43msam_training\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdefault_sam_loader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtraining_input\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_key\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*.tif\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtraining_label\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_key\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*.tif\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_segmentation_decoder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_instance_segmentation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatch_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpatch_shape_3d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbatch_size\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_seg_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_transform\u001b[49m\u001b[43m=\u001b[49m\u001b[43msam_training\u001b[49m\u001b[43m.\u001b[49m\u001b[43midentity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43msampler\u001b[49m\u001b[43m=\u001b[49m\u001b[43msampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m val_loader = sam_training.default_sam_loader(\n\u001b[32m    228\u001b[39m     raw_paths=\u001b[38;5;28mstr\u001b[39m(config[\u001b[33m\"\u001b[39m\u001b[33mval_input\u001b[39m\u001b[33m\"\u001b[39m]),\n\u001b[32m    229\u001b[39m     raw_key=\u001b[33m\"\u001b[39m\u001b[33m*.tif\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    238\u001b[39m     sampler=sampler,\n\u001b[32m    239\u001b[39m )\n\u001b[32m    241\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mData loaders created successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/omero_annotate_ai/.pixi/envs/dev/lib/python3.11/site-packages/micro_sam/training/training.py:782\u001b[39m, in \u001b[36mdefault_sam_loader\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    779\u001b[39m extra_ds_kwargs, loader_kwargs = split_kwargs(torch_em.default_segmentation_dataset, **extra_kwargs)\n\u001b[32m    780\u001b[39m ds_kwargs = {**sam_ds_kwargs, **extra_ds_kwargs}\n\u001b[32m--> \u001b[39m\u001b[32m782\u001b[39m ds = \u001b[43mdefault_sam_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mds_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch_em.segmentation.get_data_loader(ds, **loader_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/omero_annotate_ai/.pixi/envs/dev/lib/python3.11/site-packages/micro_sam/training/training.py:723\u001b[39m, in \u001b[36mdefault_sam_dataset\u001b[39m\u001b[34m(raw_paths, raw_key, label_paths, label_key, patch_shape, with_segmentation_decoder, with_channels, train_instance_segmentation_only, sampler, raw_transform, n_samples, is_train, min_size, max_sampling_attempts, rois, is_multi_tensor, **kwargs)\u001b[39m\n\u001b[32m    721\u001b[39m \u001b[38;5;66;03m# Set a minimum number of samples per epoch.\u001b[39;00m\n\u001b[32m    722\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m723\u001b[39m     loader = torch_em.default_segmentation_loader(\n\u001b[32m    724\u001b[39m         raw_paths=raw_paths,\n\u001b[32m    725\u001b[39m         raw_key=raw_key,\n\u001b[32m    726\u001b[39m         label_paths=label_paths,\n\u001b[32m    727\u001b[39m         label_key=label_key,\n\u001b[32m    728\u001b[39m         batch_size=\u001b[32m1\u001b[39m,\n\u001b[32m    729\u001b[39m         patch_shape=patch_shape,\n\u001b[32m    730\u001b[39m         with_channels=with_channels,\n\u001b[32m    731\u001b[39m         ndim=\u001b[32m2\u001b[39m,\n\u001b[32m    732\u001b[39m         is_seg_dataset=is_seg_dataset,\n\u001b[32m    733\u001b[39m         raw_transform=raw_transform,\n\u001b[32m    734\u001b[39m         rois=rois,\n\u001b[32m    735\u001b[39m         **kwargs\n\u001b[32m    736\u001b[39m     )\n\u001b[32m    737\u001b[39m     n_samples = \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(loader), \u001b[32m100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_train \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m5\u001b[39m)\n\u001b[32m    739\u001b[39m dataset = torch_em.default_segmentation_dataset(\n\u001b[32m    740\u001b[39m     raw_paths=raw_paths,\n\u001b[32m    741\u001b[39m     raw_key=raw_key,\n\u001b[32m   (...)\u001b[39m\u001b[32m    753\u001b[39m     **kwargs,\n\u001b[32m    754\u001b[39m )\n",
      "\u001b[31mTypeError\u001b[39m: torch_em.segmentation.default_segmentation_loader() got multiple values for keyword argument 'is_seg_dataset'"
     ]
    }
   ],
   "source": [
    "# âœ¨ Execute training with convenience function\n",
    "print(\"Starting micro-SAM training...\")\n",
    "\n",
    "training_results = run_training(training_config, framework=\"microsam\")\n",
    "\n",
    "print(f'ðŸŽ‰ Training completed successfully!')\n",
    "print(f'Training Results:')\n",
    "print(f'  Model name: {training_results[\"model_name\"]}')\n",
    "print(f'  Final model: {training_results.get(\"final_model_path\", \"Not available\")}')\n",
    "print(f'  Checkpoints saved: {len(training_results.get(\"checkpoints\", []))}')\n",
    "print(f'  Output directory: {training_results[\"output_dir\"]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Export and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best checkpoint from training results\n",
    "checkpoint_folder = Path(training_results.get(\"output_dir\", output_directory)) / \"checkpoints\"\n",
    "model_name = training_config[\"model_name\"]\n",
    "\n",
    "checkpoints = list(checkpoint_folder.glob(\"*.pt\")) if checkpoint_folder.exists() else []\n",
    "if checkpoints:\n",
    "    latest_checkpoint = sorted(checkpoints)[-1]\n",
    "    print(f\"Latest checkpoint: {latest_checkpoint}\")\n",
    "    \n",
    "    # Export model for inference\n",
    "    export_path = output_directory / f\"{model_name}_final.pt\"\n",
    "    print(f\"Model exported to: {export_path}\")\n",
    "else:\n",
    "    print(\"No checkpoints found.\")\n",
    "\n",
    "print(f\"\\nTraining summary:\")\n",
    "print(f\"  Output directory: {output_directory}\")\n",
    "print(f\"  Model name: {model_name}\")\n",
    "print(f\"  Dataset statistics: {training_result['stats']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micro_sam.bioimageio.model_export import export_sam_model\n",
    "import imageio.v3 as imageio\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Use the validation data from the training preparation\n",
    "val_input_path = training_result[\"val_input\"]\n",
    "val_label_path = training_result[\"val_label\"]\n",
    "\n",
    "val_images = sorted(os.listdir(val_input_path))\n",
    "val_labels = sorted(os.listdir(val_label_path))\n",
    "\n",
    "if val_images and val_labels:\n",
    "    test_image = imageio.imread(os.path.join(val_input_path, val_images[0]))\n",
    "    test_label = imageio.imread(os.path.join(val_label_path, val_labels[0]))\n",
    "    \n",
    "    bioimageio_model_path = output_directory / \"bioimage_io_model\"\n",
    "    \n",
    "    # Find the best checkpoint\n",
    "    checkpoint_dir = checkpoint_folder / model_name\n",
    "    best_checkpoint = checkpoint_dir / \"best.pt\"\n",
    "    \n",
    "    if best_checkpoint.exists():\n",
    "        export_sam_model(\n",
    "            image=test_image,\n",
    "            label_image=test_label,\n",
    "            model_type=training_config[\"model_type\"],\n",
    "            name=model_name,\n",
    "            output_path=str(bioimageio_model_path),\n",
    "            checkpoint_path=str(best_checkpoint),\n",
    "            authors=[{\"name\": \"Your Name\", \"affiliation\": \"Your Institution\"}],\n",
    "            description=\"Micro-SAM model trained on microscopy images\",\n",
    "        )\n",
    "        print(f\"BioImage.IO model exported to: {bioimageio_model_path}\")\n",
    "    else:\n",
    "        print(f\"Checkpoint not found at: {best_checkpoint}\")\n",
    "else:\n",
    "    print(\"No validation data available for model export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close OMERO connection\n",
    "if conn is not None:\n",
    "    conn.close()\n",
    "    print(\"OMERO connection closed.\")\n",
    "else:\n",
    "    print(\"No active OMERO connection to close.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
